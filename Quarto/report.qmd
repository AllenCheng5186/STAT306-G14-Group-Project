---
title: "STAT 306 2024W2 - Project: Report"
format: pdf
editor: visual
---

# Group Project Report

## Outline

1.  fit linear model with all covariates
2.  multi-collinearity check correlation among covariates (VIF)
3.  assumptions
4.  residual pattern -\> transformation
5.  model selection (forward R\^2, backward max p-value, CP)
6.  interaction term
7.  (optional) outlier, leverage, influence
8.  anova

## Data Preparation

The response variable \`SalePrice\` column is divided by 1,000, so the unit for response variable is 1 thousand dollar.

```{r}
#| echo: false
#| message: false
#| warning: false
#| output: false

library(car)
library(leaps)
library(knitr)  # TODO
library(kableExtra) # TODO
library(olsrr)

# options(repr.matrix.max.rows = 20)

ames_housing <- read.table("https://raw.githubusercontent.com/AllenCheng5186/STAT306-G14-Group-Project/refs/heads/main/data/ames-housing.csv", sep = ",", header=T)

ames_housing$SalePrice <- ames_housing$SalePrice / 10000

house = ames_housing[, c("SalePrice", "Overall.Qual", "Gr.Liv.Area",
                         "Garage.Area", "Year.Built", "Lot.Area",
                         "Bedroom.AbvGr", "Kitchen.AbvGr", "Lot.Shape",
                         "Utilities", "Yr.Sold", "Central.Air",
                         "Electrical", "TotRms.AbvGrd")]

house[sapply(house, is.character)] <- lapply(house[sapply(house, is.character)], as.factor)
house$Overall.Qual <- as.factor(house$Overall.Qual)
house[is.na(house)] <- 0

head(house)
```

## Model Selection

forward selection

```{r}
#| echo: false
#| message: false
#| warning: false
#| output: false

full_model <- lm(SalePrice ~ ., data = house)

forward <- ols_step_forward_r2(full_model)

forward$metrics
```

```{r fig.height=5,fig.width=10}
#| echo: false
#| message: false
#| warning: false
#| output: true

cp_scores <- numeric(length = length(forward$metrics$variable))
num_params <- numeric(length = length(forward$metrics$variable))
cp_distance <- numeric(length = length(forward$metrics$variable))

# Mallow's Cp
for (i in seq_along(forward$metrics$variable)) {
  predictors <- forward$metrics$variable[1:i]
  formula_str <- paste("SalePrice ~", paste(predictors, collapse = " + "))
  model_i <- lm(as.formula(formula_str), data = house)
  cp_val <- ols_mallows_cp(model_i, full_model)
  cp_scores[i] <- cp_val
  num_params[i] <- length(predictors) + 1
  cp_distance[i] = abs(cp_val - (length(predictors) + 1))
}

# AIC
predictors <- setdiff(names(house), "SalePrice")

current_formula <- as.formula("SalePrice ~ 1")
current_model <- lm(current_formula, data = house)
aic_values <- c(AIC(current_model))
num_predictors <- 0
selected <- c()

for(i in 1:length(predictors)) {
  remaining <- setdiff(predictors, selected)
  aic_candidates <- sapply(remaining, function(var) {
    new_formula <- as.formula(paste("SalePrice ~", paste(c(selected, var), collapse = " + ")))
    AIC(lm(new_formula, data = house))
  })
  best_var <- names(which.min(aic_candidates))
  selected <- c(selected, best_var)
  current_formula <- as.formula(paste("SalePrice ~", paste(selected, collapse = " + ")))
  current_model <- lm(current_formula, data = house)
  aic_values <- c(aic_values, AIC(current_model))
  num_predictors <- c(num_predictors, length(selected))
}

par(mfrow = c(1, 2))
# Plot Cp vs p+1
plot(num_params, cp_scores, type = "b", pch = 19,
     xlab = "Number of Parameters (p + 1)",
     ylab = "Mallow's Cp",
     main = "Mallow's Cp vs Number of Parameters")
abline(0, 1, col = "red", lty = 2)
abline(v = which.min(cp_distance), col = "blue", lty = 2)
# Plot AIC vs. number of predictors
plot(0:length(predictors), aic_values, type = "b",
     xlab = "Number of predictors", ylab = "AIC",
     main = "Change in AIC with Forward Selection")

min_index <- which.min(aic_values)
min_num_pred <- num_predictors[min_index]
abline(v = min_num_pred, col = "red", lty = 2)
```

These two plots shows we can use 9 covariates in our final model.

From the plot of Mallow's CP score, the score more close to number of parameter (p+1) means better

From the plot of AIC, the AIC score lower mean better.

Then we decided to use top 9 covariates from forward selection, which covariate that has strongest relationship during each iteration in the greedy algorithm

## Multi-collinearity check

calculate VIF to verify that there is not strong correlation between any continuous variables.

why use VIF? What is the advantage of using VIF? Using thumb theory of 10 as cutoff, since every continuous variable does not over 10, so no multi-collinearity.

explain what is multi-collinearity -\> any correlation among explanatory variables (X)

```{r}
#| echo: false
#| message: false
#| warning: false
#| output: true
house_numeric_only = house[, sapply(house, is.numeric)]
house_numeric_lreg = lm(SalePrice ~ ., data = house_numeric_only)

vif(house_numeric_lreg)
```

## Fit full model (using all covariates)

```{r}
#| echo: false
#| message: false
#| warning: false
#| output: false
final_model = lm(SalePrice ~ Overall.Qual + Gr.Liv.Area + Year.Built + 
                 Lot.Area + Garage.Area + Kitchen.AbvGr + Lot.Shape + 
                 Bedroom.AbvGr + Central.Air, data = house)

summary(final_model)
```

verify assumption: residual vs fitted value plot

```{r fig.height=4.5,fig.width=4.5}
#| echo: false
#| message: false
#| warning: false
#| output: true
# options(repr.plot.width = 7, repr.plot.height = 7)

plot(final_model$fitted.values, final_model$residuals,
     xlab="Fitted value", ylab="Residual")
abline(h = 0, col = "red")
```

funnel pattern appear

linear regression assumption: homoscedasticity $Var(\Sigma)  = \sigma^2$ violated

```{r fig.height=4.5,fig.width=4.5}
#| echo: false
#| message: false
#| warning: false
#| output: true
# options(repr.plot.width = 7, repr.plot.height = 7)

qqnorm(final_model$residuals)
qqline(final_model$residuals, col = "red")
```

heavy tail observed -\> homoscedasticity $Var(\Sigma) = \sigma^2$ violated

## Log(Y) Transformation

take log on response variable Y, log(Y)

create scatterplot between log(Y) and each continuous variables of X

```{r fig.height=9,fig.width=15}
#| echo: false
#| message: false
#| warning: false
#| output: true
num_vars <- names(house)[sapply(house, is.numeric)]
predictors <- setdiff(num_vars, c("SalePrice", "log_SalePrice"))

# options(repr.plot.width = 15, repr.plot.height = 10)
par(mfrow = c(2, 4))

for (var in predictors) {
  plot(house[[var]], log(house$SalePrice), pch=19,
       xlab = var, ylab = "log(SalePrice)",
       main = paste("log(SalePrice) vs", var))
  grid()
}
```

every pair show linear relationship, except for `Year.Built` that still curved.

fit linear model again with log(Y) as response variable

since `Year.Build` shows quadratic relationship, we use $$(X_{Year.Built}- \bar{X_{Year.Built}})^2$$

```{r fig.height=4.5,fig.width=4.5}
#| echo: false
#| message: false
#| warning: false
#| output: true
# options(repr.plot.width = 7, repr.plot.height = 7)

house$log_SalePrice <- log(house$SalePrice)
house$quad_Year.Built <- (house$Year.Built - mean(house$Year.Built))^2

transform_model = lm(log_SalePrice ~ Overall.Qual + Gr.Liv.Area + quad_Year.Built + 
                     Lot.Area + Garage.Area + Kitchen.AbvGr + Lot.Shape + 
                     Bedroom.AbvGr + Central.Air, data = house)

plot(x = transform_model$fitted.values, y = transform_model$residuals,
     xlim = c(1.9, 3.8), ylim =c(-1.0, 0.8),
     xlab="Fitted value", ylab="Residual")
```

looks random -\> assumptions aligned! -\> conclusion will be valid

now, transformed model become our final model.

```{r}
#| echo: false
#| message: false
#| warning: false
#| output: true
summary(transform_model)
```

Top four covariates output remain same. Inferiential question started from choose from these 4 variables.

1 Overall.Qual\
2 Gr.Liv.Area\
3 Year.Built\
4 Lot.Area

very high adjust R square

## Interaction Term

//TODO confirm if we still need to consider interaction term since all covariates in best subset model are all positive, no counterintuitive which full model has counterintuitive term.

## outlier, leverage, influence