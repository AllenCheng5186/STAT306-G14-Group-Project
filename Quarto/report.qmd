---
title: "STAT 306 2024W2 - Project: Report"
format: pdf
editor: visual
---

# Group Project Report

## Outline

1.  fit linear model with all covariates
2.  multi-collinearity check correlation among covariates (VIF)
3.  assumptions
4.  residual pattern -\> transformation
5.  model selection (forward R\^2, backward max p-value, CP)
6.  interaction term
7.  (optional) outlier, leverage, influence
8.  anova

## Data Preparation

The response variable \`SalePrice\` column is divided by 10,000, so the unit for response variable is 10 thousand dollar.

```{r}
library(car)
library(leaps)
library(knitr)
library(kableExtra)
library(olsrr)

# options(repr.matrix.max.rows = 20)

ames_housing <- read.table("https://raw.githubusercontent.com/AllenCheng5186/STAT306-G14-Group-Project/refs/heads/main/data/ames-housing.csv", sep = ",", header=T)

ames_housing$SalePrice <- ames_housing$SalePrice / 10000

house = ames_housing[, c("SalePrice","Overall.Qual", "Gr.Liv.Area", "Garage.Area", 
                         "Year.Built", "Lot.Area", "Bedroom.AbvGr", "Kitchen.AbvGr", "Lot.Shape")]

house[sapply(house, is.character)] <- lapply(house[sapply(house, is.character)], as.factor)
house$Overall.Qual <- as.factor(house$Overall.Qual)
house[is.na(house)] <- 0

head(house)
```

Based on the pairwise scatterplots for the continuous variables in the data (all variables except `Overall.Qual` and `Lot.Shape`), none of the plots show obvious linear patterns and so there do not appear to be strong linear associations.

```{r fig.height=15,fig.width=15}
options(repr.plot.width = 15, repr.plot.height = 15)

pairs(house[,-c(2, 9, 11)], pch=19)
```

There does not appear to be a strong correlation between any pair of continuous variables.

```{r}
round(cor(house[,!(names(house) %in% c("Overall.Qual","Lot.Shape"))]), 3)
```

calculate VIF to verify that there is not strong correlation between any continuous variables.

why use VIF? What is the advantage of using VIF? Using thumb theory of 10 as cutoff, since every continuous variable does not over 10, so no multi-collinearity.

explain what is multi-collinearity -\> any correlation among explanatory variables (X)

```{r}
house_numeric_only = house[, sapply(house, is.numeric)]
house_numeric_lreg = lm(SalePrice ~ ., data = house_numeric_only)

vif(house_numeric_lreg)
```

## Fit full model (using all covariates)

```{r}
full_linear_reg = lm(SalePrice~.,data = house)

summary(full_linear_reg)
```

verify assumption: residual vs fitted value plot

```{r fig.height=4.5,fig.width=4.5}
# options(repr.plot.width = 7, repr.plot.height = 7)

plot(full_linear_reg$fitted.values, full_linear_reg$residuals,
     xlab="Fitted value", ylab="Residual")
abline(h = 0, col = "red")
```

funnel pattern appear

linear regression assumption: homoscedasticity $Var(\Sigma)  = \sigma^2$ violated

```{r fig.height=4.5,fig.width=4.5}
# options(repr.plot.width = 7, repr.plot.height = 7)

qqnorm(full_linear_reg$residuals)
qqline(full_linear_reg$residuals)
```

heavy tail observed -\> homoscedasticity $Var(\Sigma) = \sigma^2$ violated

## Log(Y) Transformation

take log on response variable Y, log(Y)

create scatterplot between log(Y) and each continuous variables of X

```{r fig.height=10,fig.width=15}

num_vars <- names(house)[sapply(house, is.numeric)]
predictors <- setdiff(num_vars, c("SalePrice", "log_SalePrice"))

# options(repr.plot.width = 15, repr.plot.height = 10)
par(mfrow = c(2, 3))

for (var in predictors) {
  plot(house[[var]], log(house$SalePrice), pch=19,
       xlab = var, ylab = "log(SalePrice)",
       main = paste("log(SalePrice) vs", var))
  grid()
}
```

every pair show linear relationship, except for `Year.Built` that still curved.

fit linear model again with log(Y) as response variable

```{r fig.height=4.5,fig.width=4.5}
# options(repr.plot.width = 7, repr.plot.height = 7)

reg_logY = lm(log(SalePrice) ~ Gr.Liv.Area+ Garage.Area + Year.Built + Lot.Area + 
              Bedroom.AbvGr + Kitchen.AbvGr, data = house)

plot(x = reg_logY$fitted.values, y = reg_logY$residuals,
     xlim = c(1.9, 3.8), ylim =c(-1.0, 0.8), 
     xlab="Fitted value", ylab="Residual")
```

looks random -\> assumptions aligned! -\> conclusion will be valid

since `Year.Built` is curved which could have quadratic relationship with response variable, so fit log(Year.Built) into model again

```{r fig.height=4.5,fig.width=4.5}
reg_logY_logYear = lm(log(SalePrice) ~ Gr.Liv.Area+ Garage.Area + log(Year.Built) + 
                      Lot.Area + Bedroom.AbvGr + Kitchen.AbvGr, data = house)

plot(x = reg_logY_logYear$fitted.values, y = reg_logY_logYear$residuals,
     xlim = c(1.9, 3.8), ylim =c(-1.0, 0.8),  #TODO outlier in residual plot
     xlab="Fitted value", ylab="Residual")
```

```{r}
house$log_SalePrice = log(house$SalePrice)
house$log_Year.Built = log(house$Year.Built)

log_full_model = lm(log_SalePrice ~ Overall.Qual + Gr.Liv.Area + Garage.Area + 
                    log_Year.Built + Lot.Area + Bedroom.AbvGr + Kitchen.AbvGr + 
                    Lot.Shape, data = house)

summary(log_full_model)
```

## Model Selection

two forwarding selection functions from two different package have different result with categorical covariates.

I suggest we check with prof to decide which one use.

```{r fig.height=5,fig.width=5}
s <- regsubsets(log_SalePrice ~ Overall.Qual + Gr.Liv.Area + Garage.Area + 
                    log_Year.Built + Lot.Area + Bedroom.AbvGr + Kitchen.AbvGr + 
                    Lot.Shape, data = house, method = "forward") 

ss <- summary(s)

# s_summary <- as.data.frame(summary(s)$outmat)
# kable(s_summary, format = "latex", booktabs = TRUE) %>%
#   kable_styling(latex_options = c("scale_down", "hold_position"))

cps <- ss$cp
num_predictors <- apply(ss$which, 1, function(x) sum(x) - 1) 

model_matrix <- ss$which[, -1, drop = FALSE]  
model_names <- colnames(model_matrix)

added_vars <- character(nrow(model_matrix))
for (i in 2:nrow(model_matrix)) {
  prev <- model_matrix[i - 1, ]
  curr <- model_matrix[i, ]
  new_var <- setdiff(model_names[curr & !prev], model_names[prev])
  added_vars[i] <- ifelse(length(new_var) > 0, new_var, "")
}
added_vars[1] <- "Start"  

plot(num_predictors, cps, type = "b", pch = 19,
     xlab = "Number of Predictors", ylab = "Mallow's Cp",
     main = "Mallow's Cp vs Number of Predictors")
text(num_predictors, cps, labels = added_vars, pos = 3, cex = 0.7, col = "blue")
abline(0, 1, col = "red", lty = 2)
```

```{r}
forward_sel <- ols_step_forward_r2(log_full_model)

forward_sel$metrics
```

But top four covariates output are same

1 Overall.Qual\
2 Gr.Liv.Area\
3 Year.Built\
4 Lot.Area

Now, we fit the model with selected top 4 covariates (use log(Year.Built) for Year.Built)

```{r}
best_model = lm(log_SalePrice ~ Overall.Qual + Gr.Liv.Area + log_Year.Built + 
                Lot.Area, data = house)

summary(best_model)
```

very high adjust R square

```{r}
ols_mallows_cp(best_model, log_full_model)
```

Mallow's CP score also show it is the lowest one compared to the above plot.